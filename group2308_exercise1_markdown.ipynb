{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 1 - Group 08 (Academic Year 2022-2023)**\n",
    "\n",
    "by Erica Brisigotti (2097202), Ekaterina Chueva (???????), Sofia Pacheco Garcia (???????), Nadillia Sahputra (2070770)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0. Data generation\n",
    "First comes data generation: two sets of data are randomly generated and fed to the non-linear function which was defined externally in file <code>nonlinear_function.py</code>. The resulting dataset is made of two Numpy arrays, which is convenient since we'll be working with Keras. The data is represented to get an idea of its content and its main quantities are saved as variables (such as the number $N$ of samples and the dimension $L$ of each square sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE FOR GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the random generation of the input, we are confident that the data is already shuffled and contains no averages to be removed. Therefore we just upload it and split it into training and test sets based on a defined percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE FOR SPLITTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overrepresentation of large values, we decide to rescale each of the arrays we just obstained: this goal is achieved through the function <code>Rescale()</code>, which divides each array by the semi-length $l$ of the side of the area visible in the first example plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT Rescale FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Introduction to the model and its architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deep Neural Network is then constructed by initiating a model (through Keras' <code>Sequential()</code> function) and attaching layers to it one by one (via its method <code>add()</code> ).\n",
    "The first layer's shape must be coherent to the shape of each sample $L$, and the last layer's shape must be unitary since it returns a probability. The number of neurons for the remaining layers is arbitrary and multiple values have been considered (in this case, <code>[10, 15, 20]</code>) to ultimately find the optimal choice through a Grid Search algorithm. The total numbers of layers of the network is also an arbitrary quantity, which was set to 5.\n",
    "\n",
    "Similar considerations can be made for the choice of activation function: the activation functions for the first layers can be varied (e.g. <code>sigmoid</code>, <code>relu</code> and <code>elu</code>) to see the find the optimal choice through a Grid Search algorithm. Instead, the last layer may implement a different activation function, such as the <code>Sigmoid</code> activation function, which allows for the normalization of the output of the network and its interpretation as a probability distribution.\n",
    "\n",
    "At this step, we look out for overfitting by applying Keras' <code>Dropout()</code> at the end of the network.\n",
    "\n",
    "The choice of a <code>binary_crossentropy</code> cost function follows naturally from the nature of this classification problem, which is binary. Similarly, choosing <code>accuracy</code> (i.e.the percentage of correctly classified data points) as the metrix is a straight-forward choice for categorical tasks. \n",
    "\n",
    "The optimization through a Grid Search algorithm will also comprehend the choice of the gradient descent algorithms (e.g. <code>ADAM</code>, <code>RMSprop</code> and <code>SGD</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT create_model FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Gridsearch algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform a grid-search algorithm by variying 3 of the parameters of the model that we've already introduced \n",
    "- the activation function between <code>sigmoid</code>, <code>relu</code> and <code>elu</code>\n",
    "- the gradient descent algorithm between <code>ADAM</code>, <code>RMSprop</code> and <code>SGD</code>\n",
    "- the number of neurons for each inner layer of the network between <code>[10,15,20]</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT GridSearchCV ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance analysis based on size of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having now found the optimal combination of activation function, gradient descent algorithm and number of neurons per layer, we can further experiment with the input data to get even better performance: the strategies implemented consist of the variation of the size of the whole input data through reduction and increase, and data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSIONS ON WHAT'S BETTER BW DTAA REDUCTION/INCREASE VS AUGUMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

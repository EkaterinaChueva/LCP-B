{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 1 - Group 08 (Academic Year 2022-2023)**\n",
    "\n",
    "by Erica Brisigotti (2097202), Ekaterina Chueva (???????), Sofia Pacheco Garcia (???????), Nadillia Sahputra (???????)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we choose to work with Keras, we upload the data (based on the metadata) as two Numpy arrays, get the essential quantities (such as the number $N$ of samples and the dimension $L$ of each square sample) and represent the dataset to get an idea of its content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the example exercise (which analysed the same dataset as we are about to) we are confident that the data is already shuffled and contains no averages to be removed. Therefore we just upload it and split it into training and test sets based on a defined percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overrepresentation of large values, we decide to rescale each of the arrays we just obstained: this goal is achieved through a function, which divides each array by the semi-length $l$ of the side of the area visible in the first example plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the model and its architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deep Neural Network is then constructed by initiating a model (through Keras' <code>Sequential()</code> function) and attatching layers to it one by one (via its method <code>add()</code> ).\n",
    "The first layer's shape must be coherent to the shape of each square sample $L$, and the last layer's shape must be unitary since it returns a probability. <font color='red'>The choice of shape for the remaining ones is arbitrary</font>. <font color='red'>The total numbers of layers of the network must is also an arbitrary quantity</font>.\n",
    "\n",
    "Similar considerations can be made for the choice of activation function: <font color='red'>the activation functions for the first layers can be set based on experience after several attempt </font>. Instead, the last layer usually implements a <code>SoftMax</code> or a <code>Sigmoid</code> activation function which allows for the normalization of the output of the network, which can therefore be interpreted as a probability distribution over the predicted output classes. \n",
    "\n",
    "At this step, we look out for overfitting by applying Keras' <code>Dropout()</code> class to the last layer of the network, and settle on a choice of activation () function after several attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choosing the cost function and the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of a <code>binary_crossentropy</code> cost function follows naturally from the nature of this classification problem, which is binary. Similarly, choosing <code>accuracy</code> (i.e.the percentage of correctly classified data points) as the metrix is a straight-forward choice for categorical tasks. \n",
    "\n",
    "On the other hand, <font color='red'> the choice of the gradient descent algorithm is justified by comparison of the results shown below.</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aforemetioned model is firstly trained on minibatches (of arbitrary size <code>batch_size</code>) to prevent overfitting: when performance stabilizes, it is then trained on a relatively small, number of times (<code>nepoch</code>) on the entire dataset.\n",
    "\n",
    "The details (i.e. execution time, values of loss function and accuracy) and the identificative number of each epoch run are shown live by setting <code>verbose=2</code>. Furthermore, the values of loss function and accuracy on validation data can be calculated and shown by providing the dedicated input <code>validation_data</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating the model performance on the unseen train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then evaluate the performance of the model we just implemented on the test set: a simple approach consists of \n",
    "- a graphical representation of the accuracy and the loss, which were calculated for both the training data and the validation data and can therefore be compared\n",
    "- generating a dense grid of points onto which evaluate the model, since our classification model is relatively easy to represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modifying the hyperparameters to optimize the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TUNE PARAMETERS\n",
    "- REVISIT/BETTER JUSTIFY CHOICES OF THE SECOND PARAMTERER WHICH WAS BEFORE CHOSEN ARBITRARILY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
